{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <b> EXPERIMENTAL VERSION, NOT TESTED ON CLUSTER <b> </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate analysis using structured data - HPC4DS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<picture>\n",
    "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://docs.dask.org/en/stable/_static/images/dask-horizontal-white.svg\">\n",
    "  <img alt=\"dask'\" src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" width=\"150\">\n",
    "</picture>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student:\n",
    "- Laiti Francesco\n",
    "- Lobba Davide\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the source code to read, process and write netCDF data using Dask framework to exploit parallel computational power of the [HPC@UniTN](https://sites.google.com/unitn.it/hpc/) cluster.\n",
    "\n",
    "> The notebook has been tested on Python 3.7.7 using conda environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define the requirements to run correctly the notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare global constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the global constants used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "INPUT_FILE = '/shares/HPC4DataScience/pta/CMCC-CM2-SR5_historical/pr_day_CMCC-CM2-SR5_historical_r1i1p1f1_gn_20000101-20141231.nc'\n",
    "OUTPUT_FILE = 'pr_reduce.nc'\n",
    "\n",
    "# specific data of our problem\n",
    "NLAT = 192\n",
    "NLON = 288\n",
    "START_LAT = -90.0\n",
    "START_LON = 0.0\n",
    "LAT_NAME = \"lat\"\n",
    "LON_NAME = \"lon\"\n",
    "TIME_NAME = \"time\"\n",
    "PR_NAME = \"pr\"\n",
    "DEGREES_EAST = \"degrees_east\"\n",
    "DEGREES_NORTH = \"degrees_north\"\n",
    "LAT_UNITS = \"degrees_north\"\n",
    "LON_UNITS = \"degrees_east\"\n",
    "UNITS = \"units\"\n",
    "PR_UNITS = \"kg m-2 s-1\"\n",
    "\n",
    "# PBS confg\n",
    "QUEUE = 'short_cpuQ'\n",
    "WALLTIME = '00:02:00'\n",
    "CORES = 2 # total cores per job\n",
    "MEMORY = '2GB' # total RAM memory per job\n",
    "RESOURCE_SPEC = 'select=1:ncpus=2:mem=2gb'\n",
    "PORT_SERVER = 33567"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup PBS configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check using `qstat` if your job has been submitted. If you cannot see it, open the error output file. In my case there was an error `ImportError: cannot import name '_unicodefun' from 'click'`. To solve it install the last version of `clock` by running `pip install --upgrade click==8.0.2` from the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(cores = CORES, \n",
    "                     memory = MEMORY, \n",
    "                     resource_spec = RESOURCE_SPEC, \n",
    "                     queue = QUEUE, \n",
    "                     walltime = WALLTIME,\n",
    "                     scheduler_options={\"dashboard_address\": f\":{PORT_SERVER}\"}) # default port is already used by someone else on the cluster\n",
    "cluster.scale(jobs=5)\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.get_logs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info of netCDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nc.Dataset(INPUT_FILE)\n",
    "nrecord = ds.dimensions[TIME_NAME].size\n",
    "\n",
    "print(f'--- INFO: found dim = {len(ds.dimensions)} and nrecord = {nrecord} ---')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for the writing part\n",
    "lats = np.array(ds.variables[LAT_NAME])\n",
    "lons = np.array(ds.variables[LON_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "pr_in = da.from_array(ds.variables[PR_NAME]) # chunks='auto' is enabled by default\n",
    "pr_out = da.mean(pr_in, axis=0)\n",
    "# pr_out.visualize()\n",
    "%time pr_out = pr_out.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_in.shape)\n",
    "print(pr_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the netCDF dataset to use, just to be safe that the variable is not assigned to an open dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: ds.close()\n",
    "except: print(f\"Error while closing the {INPUT_FILE}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://unidata.github.io/python-training/workshop/Bonus/netcdf-writing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nc.Dataset(OUTPUT_FILE, mode='w', format='NETCDF4_CLASSIC')\n",
    "\n",
    "lat_dim = ds.createDimension(LAT_NAME, NLAT)\n",
    "lon_dim = ds.createDimension(LON_NAME, NLON)\n",
    "# time_dim = ds.createDimension(TIME_NAME, nrecord)\n",
    "\n",
    "ds.title = \"Average along time axis of file \" + INPUT_FILE\n",
    "\n",
    "lat = ds.createVariable(LAT_NAME, np.float32, (LAT_NAME,))\n",
    "lat.units = DEGREES_NORTH\n",
    "lat.long_name = 'latitude'\n",
    "\n",
    "lon = ds.createVariable(LON_NAME, np.float32, (LON_NAME,))\n",
    "lon.units = DEGREES_EAST\n",
    "lon.long_name = 'longitude'\n",
    "\n",
    "prec = ds.createVariable(PR_NAME, np.float32, (LAT_NAME, LON_NAME))\n",
    "prec.units = PR_UNITS\n",
    "prec.long_name = 'precipitation'\n",
    "# time = ds.createVariable('time', np.float64, ('time',))\n",
    "# time.units = 'hours since 1800-01-01'\n",
    "# time.long_name = 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlats = len(lat_dim); nlons = len(lon_dim)\n",
    "\n",
    "lat = lats\n",
    "lon = lons\n",
    "prec = pr_out\n",
    "\n",
    "print(\"-- Wrote data, prec.shape is now \", prec.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)\n",
    "\n",
    "ds.close()\n",
    "client.shutdown()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
